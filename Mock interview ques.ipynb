{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6abd790b-7058-4511-a156-b2370abfdc13",
   "metadata": {},
   "source": [
    "## 50 Core Technical Questionnaires with their answers that you should know before doing the Mock Interviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83eea2e8-3507-4214-a3a0-de118ccf7cb9",
   "metadata": {},
   "source": [
    "### Interview Questions based on the projects mentioned in the resume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25eae97c-c619-45f7-abbf-d0926d6422f4",
   "metadata": {},
   "source": [
    "### 1. Here are some tailored interview questions and answers based on a project where I built a comprehensive revenue forecasting model using Python and scikit-learn:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91e992d-4b9e-4708-bf3e-82093be8d233",
   "metadata": {},
   "source": [
    "### Project Understanding and Overview\n",
    "\n",
    "#### 1. Can you explain the objective of your revenue forecasting project?\n",
    "#### \t•\tAnswer:\n",
    "#### \t•\tThe objective was to build a machine learning model that accurately predicts future revenue based on historical data, seasonal trends, and other influential factors. This helps businesses optimize resource allocation, inventory management, and strategic planning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3881e23d-bee1-424d-a2d9-bb91eab3dedc",
   "metadata": {},
   "source": [
    "#### 2. What were the key steps in building the forecasting model?\n",
    "#### \t•\tAnswer:\n",
    "#### \t1.\tUnderstanding the problem and gathering requirements.\n",
    "#### \t2.\tCollecting and preprocessing historical data.\n",
    "#### \t3.\tPerforming exploratory data analysis (EDA) to uncover trends, seasonality, and anomalies.\n",
    "#### \t4.\tFeature engineering to derive meaningful predictors (e.g., lag features, moving averages).\n",
    "#### \t5.\tSplitting data into training, validation, and test sets.\n",
    "#### \t6.\tSelecting and tuning machine learning models (e.g., linear regression, decision trees, or ensemble methods).\n",
    "#### \t7.\tEvaluating model performance using metrics like MAE, RMSE, and MAPE.\n",
    "#### \t8.\tDeploying the model for real-time or batch predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0669fe7-87d5-435f-83c3-e7edf873c81d",
   "metadata": {},
   "source": [
    "### Data Handling and Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea586ec4-78f2-49c5-9eb1-05a49436de16",
   "metadata": {},
   "source": [
    "#### 3. What kind of data did you use for revenue forecasting?\n",
    "#### \t•\tAnswer:\n",
    "#### \t•\tHistorical revenue data.\n",
    "#### \t•\tTime-related features (e.g., day of the week, month, year).\n",
    "#### \t•\tExternal variables (e.g., economic indicators, holidays, weather conditions).\n",
    "#### \t•\tDerived features like lag revenue, moving averages, and growth rates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fe5aea-2c08-4028-9308-607c525aa97a",
   "metadata": {},
   "source": [
    "#### 4. How did you handle missing data in your dataset?\n",
    "#### \t•\tAnswer:\n",
    "#### \t•\tDepending on the context:\n",
    "#### \t•\tImputed missing values with mean/median for numerical features or mode for categorical features.\n",
    "#### \t•\tUsed forward-fill/backward-fill for time-series data.\n",
    "#### \t•\tDropped records if the percentage of missing data was significant and imputation wasn’t feasible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b745d15-3cde-4c1a-8c8a-ebcd15fc8897",
   "metadata": {},
   "source": [
    "#### 5. What feature engineering techniques did you apply?\n",
    "#### \t•\tAnswer:\n",
    "#### \t•\tCreated lag features to capture revenue from previous periods.\n",
    "#### \t•\tGenerated rolling statistics (e.g., moving averages, rolling standard deviation).\n",
    "#### \t•\tEncoded time-related features like day of the week, month, and quarter.\n",
    "#### \t•\tDerived external factors such as holiday indicators and promotion effects.\n",
    "#### \t•\tScaled numerical features using StandardScaler or MinMaxScaler for models sensitive to scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4f83dd-7986-425a-907d-16ddc12915b6",
   "metadata": {},
   "source": [
    "### Model Selection and Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6125b3-e525-4c10-a422-e2f04658bc18",
   "metadata": {},
   "source": [
    "#### 6. Which machine learning algorithms did you try, and why?\n",
    "#### \t•\tAnswer:\n",
    "#### \t•\tLinear Regression: To establish a baseline and for its interpretability.\n",
    "#### \t•\tRandom Forest/Gradient Boosting (e.g., XGBoost, LightGBM): To handle non-linear relationships and interactions between features.\n",
    "\n",
    "\t\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520ad57d-d304-4faf-8506-2bc4e55df26c",
   "metadata": {},
   "source": [
    "#### 7. What metrics did you use to evaluate model performance, and why?\n",
    "#### \t•\tAnswer:\n",
    "#### \t•\tMean Absolute Error (MAE): Easy to interpret and robust to outliers.\n",
    "#### \t•\tRoot Mean Squared Error (RMSE): Penalizes large errors more heavily, useful for emphasizing precision.\n",
    "#### \t•\tMean Absolute Percentage Error (MAPE): Provides a percentage error for interpretability.\n",
    "#### \t•\tUsed cross-validation and backtesting on time-series data to ensure robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1707efc-38c8-47e2-be32-88ec338475bb",
   "metadata": {},
   "source": [
    "#### 8. How did you handle overfitting in your model?\n",
    "#### \t•\tAnswer:\n",
    "#### \t•\tUsed techniques such as:\n",
    "#### \t•\tRegularization (e.g., Ridge, Lasso).\n",
    "#### \t•\tPruning (for tree-based models).\n",
    "#### \t•\tCross-validation to ensure generalization.\n",
    "#### \t•\tFeature selection to remove irrelevant features.\n",
    "#### \t•\tTuning hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcf190d-bf21-406a-a505-2cc24a1c6b75",
   "metadata": {},
   "source": [
    "#### 9. Did you encounter any challenges while working on this project? How did you resolve them?\n",
    "#### \t•\tAnswer:\n",
    "#### \t•\tChallenge 1: Insufficient data for specific periods.\n",
    "#### Solution: Augmented data with external datasets or used interpolation techniques.\n",
    "#### \t•\tChallenge 2: High variance in revenue data.\n",
    "#### Solution: Applied smoothing techniques and used robust algorithms like gradient boosting.\n",
    "#### \t•\tChallenge 3: Model drift in real-time predictions.\n",
    "#### Solution: Monitored model performance and implemented periodic retraining."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25a0e63-15de-458a-8262-97eac37f42fc",
   "metadata": {},
   "source": [
    "### Deployment and Business Impact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093a881d-8ed0-4ac6-9eba-813ad73738d6",
   "metadata": {},
   "source": [
    "#### 10. What was the business impact of your model?\n",
    "#### \t•\tAnswer:\n",
    "#### \t•\tImproved revenue prediction accuracy by 15%.\n",
    "#### \t•\tEnabled better inventory and resource management, reducing costs by 20%.\n",
    "#### \t•\tEnhanced decision-making for promotions and pricing strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb257a0-3825-4a3a-8aa9-e416bed77913",
   "metadata": {},
   "source": [
    "### General ETL Process Questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c748e7-a63d-42a8-b90c-f8a8e6b79463",
   "metadata": {},
   "source": [
    "#### 11. What does ETL stand for, and how did you implement it in your project?\n",
    "#### \t•\tAnswer:\n",
    "#### \t•\tETL stands for Extract, Transform, and Load:\n",
    "#### \t1.\tExtract: Loaded data from various sources like CSV files, databases, and APIs using libraries like pandas.read_csv().\n",
    "#### \t2.\tTransform: Cleansed, aggregated, and transformed data using pandas operations such as .groupby(), .merge(), and .apply().\n",
    "#### \t3.\tLoad: Saved the processed data back to databases or files using to_csv() or database connectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec48f6f3-fd5a-4277-b86b-510498e10857",
   "metadata": {},
   "source": [
    "#### 12. How did you handle data extraction from multiple sources?\n",
    "#### \t•\tAnswer:\n",
    "#### \t•\tConnected to relational databases using SQLAlchemy.\n",
    "#### \t•\tExtracted data from REST APIs using requests library and processed JSON responses.\n",
    "#### \t•\tUsed pandas functions like read_csv(), read_excel(), or read_sql_query() for file-based and SQL-based extractions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f619001-8b63-45de-812b-cb1db0e4cd81",
   "metadata": {},
   "source": [
    "#### 13. How do you ensure data consistency during extraction?\n",
    "#### \t•\tAnswer:\n",
    "#### \t•\tValidated source data formats using pandas validation techniques (e.g., .dtypes checks).\n",
    "#### \t•\tHandled schema differences between sources by normalizing column names and data types.\n",
    "#### \t•\tLogged extraction errors for records that couldn’t be processed, ensuring they could be reviewed and reprocessed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed7fa84-76ed-464c-b1fe-e67332e046ba",
   "metadata": {},
   "source": [
    "#### 14. What transformations did you perform on the raw data?\n",
    "#### \t•\tAnswer:\n",
    "#### \t•\tRemoved duplicates using drop_duplicates().\n",
    "#### \t•\tHandled missing data by:\n",
    "#### \t•\tFilling with mean/median/mode using fillna().\n",
    "#### \t•\tDropping rows/columns using dropna().\n",
    "#### \t•\tNormalized and standardized data with pandas and sklearn.preprocessing.\n",
    "#### \t•\tMerged datasets using merge() and joined tables with common keys.\n",
    "#### \t•\tAggregated data using .groupby() to compute metrics like sum, average, and counts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faeb73a-ba44-4dbf-b8cd-f13c893935b4",
   "metadata": {},
   "source": [
    "#### 15. How did you ensure data quality during transformation?\n",
    "#### \t•\tAnswer:\n",
    "#### \t•\tApplied data validation rules like checking for outliers and setting constraints for acceptable ranges.\n",
    "#### \t•\tUsed assertions in Python to enforce rules, e.g., assert df['value'].notnull().all().\n",
    "#### \t•\tApplied regular expressions (str.contains() or str.match()) to validate string patterns like email or phone formats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d3277b-c08b-41ac-abab-8ac42fe84b7c",
   "metadata": {},
   "source": [
    "#### 16. What steps did you take to optimize the data loading process?\n",
    "#### \t•\tAnswer:\n",
    "#### \t•\tUsed bulk inserts when loading data into databases to reduce transaction overhead.\n",
    "#### \t•\tCompressed large datasets into .parquet format for efficient storage and retrieval.\n",
    "#### \t•\tEnsured transactional integrity by committing or rolling back changes in case of failures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badedf5b-79bc-40f9-a13f-e27bdaed38c4",
   "metadata": {},
   "source": [
    "#### 17. How did you load the cleansed data into the destination?\n",
    "#### \t•\tAnswer:\n",
    "#### \t•\tUsed to_csv() and to_parquet() for saving data to files.\n",
    "#### \t•\tUsed to_sql() with SQLAlchemy to insert data into relational databases.\n",
    "#### \t•\tScheduled batch processing and automated loading with tools like Apache Airflow or cron jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d28a06-3a3c-43c2-b6e7-f91dd0f19563",
   "metadata": {},
   "source": [
    "#### 18. How did you handle performance issues with pandas while processing large datasets?\n",
    "#### \t•\tAnswer:\n",
    "#### \t•\tLeveraged pandas chunking (chunksize) to process data in smaller portions.\n",
    "####    •   Used parallel processing libraries like dask or modin for distributed data processing.\n",
    "#### \t•\tReduced memory footprint by explicitly specifying data types (pd.to_numeric() or .astype()).\n",
    "#### \t•\tReplaced loops with vectorized operations and used .applymap() instead of .apply() when applying element-wise functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea5b72b-0485-46f5-bbc1-ff4a55986538",
   "metadata": {},
   "source": [
    "### Interactive Dashboard Tableau Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7db3ec-aa26-4811-9f20-9bb1b3cb14b7",
   "metadata": {},
   "source": [
    "#### 19. What are the key steps involved in creating an interactive Tableau dashboard?\n",
    "#### \t•\tAnswer:\n",
    "#### \t1.\tConnect to a data source and import data into Tableau.\n",
    "#### \t2.\tPrepare and clean the data using Tableau Prep or in Tableau Desktop.\n",
    "#### \t3.\tCreate individual visualizations (e.g., charts, maps).\n",
    "#### \t4.\tCombine visualizations into a dashboard.\n",
    "#### \t5.\tAdd interactivity with filters, actions, and parameters.\n",
    "#### \t6.\tCustomize the layout for user experience (e.g., responsive design).\n",
    "#### \t7.\tPublish the dashboard to Tableau Server or Tableau Online for sharing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b661b59-f86a-449d-8d04-95bbc8530d4a",
   "metadata": {},
   "source": [
    "#### 20. What types of filters can you use in a Tableau dashboard?\n",
    "#### \t•\tAnswer:\n",
    "#### \t•\tQuick Filters: Allow users to filter views interactively.\n",
    "#### \t•\tGlobal Filters: Apply filters across multiple worksheets on the dashboard.\n",
    "#### \t•\tContext Filters: Restrict the data shown in other filters.\n",
    "#### \t•\tExtract Filters: Filter data while creating an extract to improve performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2933a6bf-c7e4-4140-9b55-680e279dc285",
   "metadata": {},
   "source": [
    "#### 21. How do you add interactivity to a Tableau dashboard?\n",
    "#### \t•\tAnswer:\n",
    "#### \t•\tUse filters to allow users to narrow down data.\n",
    "#### \t•\tAdd dashboard actions like:\n",
    "#### \t•\tFilter Actions: Dynamically filter data across sheets.\n",
    "#### \t•\tHighlight Actions: Highlight related data in other views.\n",
    "#### \t•\tURL Actions: Link to external web pages.\n",
    "#### \t•\tUse parameters to let users dynamically change input values.\n",
    "#### \t•\tEnable drill-downs by organizing data hierarchically (e.g., Category → Subcategory).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cac54a4-839d-4cd2-beab-bfb42b1c539b",
   "metadata": {},
   "source": [
    "#### 22. How do you ensure your Tableau dashboard is user-friendly?\n",
    "#### \t•\tAnswer:\n",
    "#### \t•\tKeep the layout simple and intuitive with a clear visual hierarchy.\n",
    "#### \t•\tUse consistent color schemes and labels.\n",
    "#### \t•\tInclude tooltips and legends for better context.\n",
    "#### \t•\tAdd titles and descriptions to guide users.\n",
    "#### \t•\tTest the dashboard with end-users to gather feedback on usability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba8c47e-3d3a-44aa-8a6c-8f8879dd0563",
   "metadata": {},
   "source": [
    "### SQL Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6be112-46f6-4989-8414-5e72633fe913",
   "metadata": {},
   "source": [
    "#### 23. What is SQL, and what are its main types of commands?\n",
    "#### \t•\tAnswer:\n",
    "#### SQL (Structured Query Language) is used to communicate with and manage databases. It includes:\n",
    "#### \t•\tDDL (Data Definition Language): CREATE, ALTER, DROP\n",
    "#### \t•\tDML (Data Manipulation Language): INSERT, UPDATE, DELETE\n",
    "#### \t•\tDQL (Data Query Language): SELECT\n",
    "#### \t•\tTCL (Transaction Control Language): COMMIT, ROLLBACK, SAVEPOINT\n",
    "#### \t•\tDCL (Data Control Language): GRANT, REVOKE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993ae494-a754-4771-8ab5-bd59feb5cfbf",
   "metadata": {},
   "source": [
    "#### 24. How do you retrieve unique values from a column?\n",
    "#### \t•\tQuery:\n",
    "        SELECT DISTINCT column_name\n",
    "        FROM table_name;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14469609-1dc0-4439-8707-d966932511d3",
   "metadata": {},
   "source": [
    "#### 25. How do you write an SQL query to find the second highest salary?\n",
    "#### \t•\tQuery:\n",
    "             SELECT MAX(salary) AS second_highest_salary\n",
    "             FROM employees\n",
    "             WHERE salary < (SELECT MAX(salary) FROM employees);\n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a24d914-8590-4b0d-a6fa-3798c553f7e9",
   "metadata": {},
   "source": [
    "#### 26. Explain the difference between INNER JOIN, LEFT JOIN, and RIGHT JOIN.\n",
    "#### \t•\tAnswer:\n",
    "#### \t•\tINNER JOIN: Returns rows with matching values in both tables.\n",
    "#### \t•\tLEFT JOIN: Returns all rows from the left table and matching rows from the right table.\n",
    "#### \t•\tRIGHT JOIN: Returns all rows from the right table and matching rows from the left table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448512d9-e4b5-40a7-8759-ae193f01611e",
   "metadata": {},
   "source": [
    "#### 27. What is a subquery, and how is it used?\n",
    "#### \t•\tAnswer:\n",
    "#### A subquery is a query within another query, often used to filter results.\n",
    "#### \t•\tExample:\n",
    "              SELECT employee_name\n",
    "              FROM employees\n",
    "              WHERE department_id = (\n",
    "                  SELECT department_id\n",
    "                  FROM departments\n",
    "                  WHERE department_name = 'IT'\n",
    "                  );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16106d5-7804-40f0-a9d1-4a3e67dc43d1",
   "metadata": {},
   "source": [
    "#### 28. Explain window functions in SQL.\n",
    "#### \t•\tAnswer:\n",
    "#### Window functions perform calculations across rows related to the current row without collapsing data.\n",
    "#### \t•\tExample:\n",
    "                    SELECT employee_name, department_id, salary,\n",
    "                           SUM(salary) OVER (PARTITION BY department_id) AS department_total\n",
    "                    FROM employees;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afc5a48-22ed-4e69-b83e-2530c1a8a6c0",
   "metadata": {},
   "source": [
    "#### 29. Explain the difference between UNION and UNION ALL.\n",
    "#### \t•\tAnswer:\n",
    "#### \t•\tUNION: Combines results from two queries and removes duplicates.\n",
    "#### \t•\tUNION ALL: Combines results and includes duplicates.\n",
    "#### \t•\tExample:\n",
    "               -- Using UNION\n",
    "              SELECT employee_name FROM employees_2023\n",
    "              UNION\n",
    "              SELECT employee_name FROM employees_2022;\n",
    "\n",
    "              -- Using UNION ALL\n",
    "             SELECT employee_name FROM employees_2023\n",
    "             UNION ALL\n",
    "             SELECT employee_name FROM employees_2022;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddafb94-592d-428a-ae20-0e0ba95d0763",
   "metadata": {},
   "source": [
    "#### Basic MySQL Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbf463c-b7b6-4176-895d-8844e259e41e",
   "metadata": {},
   "source": [
    "#### 30. What is MySQL?\n",
    "#### \t•\tAnswer:\n",
    "#### MySQL is an open-source relational database management system (RDBMS) based on Structured Query Language (SQL). It is widely used for web applications and supports various operating systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1b6784-b962-490f-aede-86a6c0a0af27",
   "metadata": {},
   "source": [
    "#### 31. What are the different storage engines in MySQL?\n",
    "#### \t•\tAnswer:\n",
    "#### MySQL supports several storage engines, including:\n",
    "#### \t•\tInnoDB: Default engine, supports transactions and foreign keys.\n",
    "#### \t•\tMyISAM: Fast for read-heavy workloads but lacks transactions.\n",
    "#### \t•\tMemory: Stores data in RAM for faster access.\n",
    "#### \t•\tCSV: Stores data in plain text files in CSV format.\n",
    "#### \t•\tArchive: Optimized for high data compression and storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce70ea5-1f90-4273-81b2-8d65d97a7986",
   "metadata": {},
   "source": [
    "#### 32. What are MySQL’s constraints?\n",
    "#### \t•\tAnswer:\n",
    "#### Constraints are rules enforced on table columns to ensure data integrity:\n",
    "#### \t•\tNOT NULL: Prevents null values.\n",
    "#### \t•\tUNIQUE: Ensures unique values in a column.\n",
    "#### \t•\tPRIMARY KEY: Combines NOT NULL and UNIQUE.\n",
    "#### \t•\tFOREIGN KEY: Establishes relationships between tables.\n",
    "#### \t•\tCHECK: Ensures values meet specific conditions.\n",
    "#### \t•\tDEFAULT: Assigns default values to a column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c49e617-e970-43d4-a115-ae6385827af1",
   "metadata": {},
   "source": [
    "### Regression Analysis Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff03b271-71bd-4adc-80bd-55dfa29d7fef",
   "metadata": {},
   "source": [
    "#### 33. What is regression analysis?\n",
    "#### \t•\tAnswer:\n",
    "#### Regression analysis is a statistical method used to determine the relationship between a dependent variable (target) and one or more independent variables (predictors). It is widely used for prediction, forecasting, and understanding the relationships in data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5112125b-7669-495a-8242-eeab07071816",
   "metadata": {},
   "source": [
    "#### 34. What is the difference between linear regression and logistic regression?\n",
    "#### \t•\tAnswer:\n",
    "#### \t•\tLinear Regression:\n",
    "#### \t•\tPredicts continuous outcomes.\n",
    "#### \t•\tOutput: Real numbers (e.g., sales, temperature).\n",
    "#### \t•\tEquation:  Y = \\beta_0 + \\beta_1X + \\epsilon .\n",
    "#### \t•\tLogistic Regression:\n",
    "#### \t•\tPredicts binary or categorical outcomes.\n",
    "#### \t•\tOutput: Probabilities (e.g., spam/not spam).\n",
    "#### \t•\tEquation:  P(Y=1) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1X)}} ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf687f8-db3b-4dc0-b29f-705cb8d8568a",
   "metadata": {},
   "source": [
    "#### 35. How do you evaluate the performance of a regression model?\n",
    "#### \t•\tAnswer:\n",
    "#### Common metrics include:\n",
    "#### \t•\tMean Absolute Error (MAE):  \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i| \n",
    "#### \t•\tMean Squared Error (MSE):  \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \n",
    "#### \t•\tRoot Mean Squared Error (RMSE):  \\sqrt{MSE} \n",
    "#### \t•\tR-Squared (Coefficient of Determination): Proportion of variance explained by the model.\n",
    "#### \t•\tAdjusted R-Squared: Adjusted for the number of predictors in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee02232-a9c2-45e9-818d-e7ba693a6b10",
   "metadata": {},
   "source": [
    "#### 36. What is regularization in regression, and why is it important?\n",
    "#### \t•\tAnswer:\n",
    "#### Regularization adds a penalty term to the loss function to reduce overfitting and improve generalization:\n",
    "#### \t•\tL1 Regularization (Lasso): Shrinks coefficients and can set some to zero, effectively performing feature selection.\n",
    "#### \t•\tL2 Regularization (Ridge): Shrinks coefficients but does not set any to zero.\n",
    "#### \t•\tLasso Loss Function:  \\text{Loss} = \\sum_{i=1}^{n} (y_i - \\hat{y}i)^2 + \\lambda \\sum{j=1}^{p} |\\beta_j| \n",
    "####    •\tRidge Loss Function:  \\text{Loss} = \\sum_{i=1}^{n} (y_i - \\hat{y}i)^2 + \\lambda \\sum{j=1}^{p} \\beta_j^2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17eecac6-f144-4e10-a797-4f88ac4b89d6",
   "metadata": {},
   "source": [
    "#### 37. What are outliers, and how do you handle them in regression?\n",
    "#### \t•\tAnswer:\n",
    "#### \t•\tOutliers are extreme values that deviate significantly from the rest of the data.\n",
    "#### \t•\tHandling methods:\n",
    "#### \t•\tRemove if they are errors.\n",
    "#### \t•\tUse robust regression techniques.\n",
    "#### \t•\tApply transformations (e.g., log, square root)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce9faaa-ceba-43b6-9292-0691ff192c37",
   "metadata": {},
   "source": [
    "#### 38. How would you explain a low R-Squared value in your model?\n",
    "#### \t•\tAnswer:\n",
    "#### \t•\tA low R-Squared may indicate:\n",
    "#### \t•\tPoor predictors.\n",
    "#### \t•\tHigh variability in data not captured by predictors.\n",
    "#### \t•\tSteps to address:\n",
    "#### \t•\tAdd more relevant features.\n",
    "#### \t•\tUse polynomial or interaction terms.\n",
    "#### \t•\tCheck for nonlinear relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344dbbd8-b600-43a5-a7ff-b81f3215da9f",
   "metadata": {},
   "source": [
    "#### 39. Explain the steps to build and evaluate a regression model.\n",
    "#### \t•\tAnswer:\n",
    "#### \t1.\tData Exploration: Understand the data, check distributions, and identify outliers/missing values.\n",
    "#### \t2.\tFeature Engineering: Create or transform features.\n",
    "#### \t3.\tModel Building: Fit a regression model (linear, logistic, etc.).\n",
    "#### \t4.\tModel Evaluation: Use metrics like RMSE, R-Squared, or MAE.\n",
    "#### \t5.\tInterpretation: Analyze coefficients and residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ace88f6-556b-4263-bec6-90d091d3364a",
   "metadata": {},
   "source": [
    "#### 40. What is overfitting and how to handle it?\n",
    "#### Answer:\n",
    "#### Overfitting occurs when a model learns the noise and details of the training data too well, leading to poor generalization on unseen data. The model is too complex relative to the dataset.\n",
    "#### Ways to handle the overfitting\n",
    "#### 1.\tReduce Model Complexity:\n",
    "#### \t•\tSimplify the model by reducing the number of predictors (feature selection).\n",
    "#### \t•\tRemove polynomial or interaction terms if they are unnecessary.\n",
    "#### \t2.\tRegularization:\n",
    "#### \t•\tUse techniques like L1 (Lasso) or L2 (Ridge) regularization to penalize large coefficients and prevent overfitting.\n",
    "#### \t3.\tCross-Validation:\n",
    "#### \t•\tUse cross-validation (e.g., k-fold) to evaluate model performance and detect overfitting during training.\n",
    "#### \t4.\tUse More Training Data:\n",
    "#### \t•\tIncrease the size of the training set, if possible, to provide the model with more diverse examples.\n",
    "#### \t5.\tDrop Irrelevant Features:\n",
    "#### \t•\tUse feature selection methods like Recursive Feature Elimination (RFE) or Lasso to remove features that don’t contribute meaningfully.\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95eb1037-dfbf-4466-b479-e92db8ca754e",
   "metadata": {},
   "source": [
    "#### 41. What is Underfitting and how to handle it?\n",
    "#### Answer:\n",
    "#### Underfitting happens when the model is too simple to capture the underlying structure of the data. It performs poorly on both the training and test datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cc9adc-5dbd-451e-bb56-974b86bcb18e",
   "metadata": {},
   "source": [
    "#### Ways to handle the underfitting\n",
    "#### \t1.\tIncrease Model Complexity:\n",
    "#### \t•\tAdd more features or predictors that are relevant to the target variable.\n",
    "#### \t•\tInclude polynomial or interaction terms to capture nonlinear relationships.\n",
    "#### \t2.\tReduce Regularization:\n",
    "#### \t•\tIf using regularization, decrease the penalty to allow the model more flexibility.\n",
    "#### Example:\n",
    "             from sklearn.linear_model import Lasso\n",
    "             model = Lasso(alpha=0.01)  # Lower alpha reduces regularization\n",
    "             model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc1c24e-ec37-4a33-8c98-0f99f96f83d5",
   "metadata": {},
   "source": [
    "#### \t3.\tUse a More Sophisticated Model:\n",
    "#### \t•\tReplace a simple model (e.g., linear regression) with a more complex one (e.g., decision trees, random forests, or gradient boosting).\n",
    "#### \t4.\tTune Hyperparameters:\n",
    "#### \t•\tOptimize parameters of the regression algorithm to improve performance.\n",
    "#### •\tExample (using GridSearchCV):\n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "        param_grid = {'n_estimators': [50, 100, 150], 'max_depth': [None, 10, 20]}\n",
    "        grid_search = GridSearchCV(RandomForestRegressor(), param_grid, cv=5)\n",
    "        grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f2bbef-796e-43d4-bed2-bf241cc1d089",
   "metadata": {},
   "source": [
    "### AWS RedShift Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89d9461-c683-463f-860a-6089cd1a266b",
   "metadata": {},
   "source": [
    "#### 41. What is Amazon Redshift?\n",
    "#### \t•\tAnswer:\n",
    "#### Amazon Redshift is a fully managed, petabyte-scale data warehouse service in the cloud. It allows organizations to run complex analytical queries against structured and semi-structured data stored across data warehouses, operational databases, and data lakes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af1244c-99ef-4ae4-bd0a-a80a53445855",
   "metadata": {},
   "source": [
    "#### 42. How does Amazon Redshift store data?\n",
    "#### \t•\tAnswer:\n",
    "#### \t•\tRedshift uses columnar storage, where data is stored by columns instead of rows. This enables faster analytical queries since only the relevant columns are read for a query.\n",
    "#### \t•\tData is distributed across nodes using distribution styles: EVEN, KEY, and ALL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9bd4e2-87fa-4c91-8c4e-c8f0aad7a95b",
   "metadata": {},
   "source": [
    "#### 43. What are distribution styles in Redshift, and when do you use them?\n",
    "#### \t•\tAnswer:\n",
    "#### \t•\tEVEN: Distributes rows evenly across all nodes. Best for tables with no clear join keys.\n",
    "#### \t•\tKEY: Distributes rows based on the value of a distribution key. Best for frequently joined tables to minimize data shuffling.\n",
    "#### \t•\tALL: Replicates the entire table to all nodes. Best for small dimension tables frequently joined with other tables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b283debe-6fac-404d-9258-38066569182b",
   "metadata": {},
   "source": [
    "#### 44. How does Redshift handle backup and recovery?\n",
    "#### \t•\tAnswer:\n",
    "#### \t•\tRedshift performs automatic snapshots of the cluster and saves them to Amazon S3.\n",
    "#### \t•\tManual Snapshots: Users can create manual snapshots that persist even if the cluster is deleted.\n",
    "#### \t•\tCross-Region Snapshots: Snapshots can be copied to a different region for disaster recovery."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72ae0c0-b6f8-43fd-a013-90ae1375f12d",
   "metadata": {},
   "source": [
    "### Questions on Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5bedc3-7aa8-49ee-be1f-7e9c878ec3f8",
   "metadata": {},
   "source": [
    "#### 45. What is Exploratory Data Analysis (EDA), and why is it important?\n",
    "#### Answer:\n",
    "#### \t•\tEDA is the process of analyzing datasets to summarize their main characteristics, often with visualizations, to uncover patterns, anomalies, and relationships in the data.\n",
    "#### \t•\tImportance:\n",
    "#### \t•\tUnderstand data distribution, quality, and structure.\n",
    "#### \t•\tIdentify outliers and missing values.\n",
    "#### \t•\tSelect appropriate models and transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf577f6-7227-40a4-83fe-3361e9883948",
   "metadata": {},
   "source": [
    "#### 46. What are the key steps in performing EDA?\n",
    "#### Answer:\n",
    "#### \t•\tUnderstand the data context and objectives.\n",
    "#### \t•\tLoad and inspect the data (e.g., data types, dimensions).\n",
    "#### \t•\tCheck for missing values and duplicates.\n",
    "#### \t•\tAnalyze statistical summaries (mean, median, variance).\n",
    "#### \t•\tVisualize distributions, correlations, and trends (e.g., histograms, scatter plots).\n",
    "#### \t•\tHandle outliers and missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5300be3-af01-4b2c-b1b0-98c7780c4c30",
   "metadata": {},
   "source": [
    "#### 47. Which Python libraries are commonly used for EDA?\n",
    "#### Answer:\n",
    "#### \t•\tPandas: Data manipulation and analysis.\n",
    "#### \t•\tNumPy: Numerical operations.\n",
    "#### \t•\tMatplotlib/Seaborn: Data visualization.\n",
    "#### \t•\tSciPy: Statistical analysis.\n",
    "#### \t•\tPlotly: Interactive visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6271dfa-9456-4312-9d21-388d188985d3",
   "metadata": {},
   "source": [
    "#### 48. How do you handle missing data during EDA?\n",
    "#### Answer:\n",
    "#### \t•\tImputation: Fill missing values using techniques like mean, median, or mode.\n",
    "#### \t•\tDeletion: Remove rows/columns with a high percentage of missing values.\n",
    "#### \t•\tModel-Based Imputation: Predict missing values using machine learning models.\n",
    "#### \t•\tIndicator Variable: Create a separate binary column to indicate missingness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81c6bcc-57cc-4b09-9232-7ade4f65b559",
   "metadata": {},
   "source": [
    "#### 49. How do you detect and handle outliers in data?\n",
    "#### Answer:\n",
    "#### \t•\tDetection Methods:\n",
    "#### \t•\tBox plots, scatter plots, z-scores, or the IQR method.\n",
    "#### \t•\tHandling Outliers:\n",
    "#### \t•\tRemove: If the outliers are errors or irrelevant.\n",
    "#### \t•\tTransform: Use log or square root transformations to reduce their impact.\n",
    "#### \t•\tCap: Winsorize the data to limit extreme values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaf041d-c05c-45fb-8b3b-25fef4c2c300",
   "metadata": {},
   "source": [
    "#### 50. How do you detect and handle outliers in data?\n",
    "#### Answer:\n",
    "#### \t•\tDetection Methods:\n",
    "#### \t•\tBox plots, scatter plots, z-scores, or the IQR method.\n",
    "#### \t•\tHandling Outliers:\n",
    "#### \t•\tRemove: If the outliers are errors or irrelevant.\n",
    "#### \t•\tTransform: Use log or square root transformations to reduce their impact.\n",
    "#### \t•\tCap: Winsorize the data to limit extreme values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6da605-b871-4fb0-8caa-6a409b26ad04",
   "metadata": {},
   "source": [
    "#### 51. What is the difference between univariate, bivariate, and multivariate analysis?\n",
    "#### Answer:\n",
    "#### \t•\tUnivariate Analysis: Examines one variable at a time (e.g., histograms, box plots).\n",
    "#### \t•\tBivariate Analysis: Examines the relationship between two variables (e.g., scatter plots, correlation matrices).\n",
    "#### \t•\tMultivariate Analysis: Explores relationships among more than two variables (e.g., pair plots, PCA)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b124b82b-4c0f-4072-86d6-32f3e6e5217c",
   "metadata": {},
   "source": [
    "#### 52. How do you check for multicollinearity in a dataset?\n",
    "#### Answer:\n",
    "#### \t•\tVariance Inflation Factor (VIF): A high VIF (>10) indicates multicollinearity.\n",
    "#### \t•\tCorrelation Matrix: Check for high correlations between predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aacf177-94cc-442e-8679-99610209800b",
   "metadata": {},
   "source": [
    "#### 53. How do you handle high-dimensional datasets during EDA?\n",
    "#### Answer:\n",
    "#### \t•\tUse dimensionality reduction techniques like PCA or t-SNE.\n",
    "#### \t•\tPerform feature selection using correlation analysis, feature importance, or recursive feature elimination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d23374-64ce-458a-94fc-1f38ea9c12a8",
   "metadata": {},
   "source": [
    "### Questions on Data Governance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44750053-6ea8-4735-b6ed-04f8a2f609dc",
   "metadata": {},
   "source": [
    "#### 54. What is data governance, and why is it important?\n",
    "#### Answer:\n",
    "#### \t•\tData governance is the practice of managing data’s availability, usability, integrity, and security within an organization.\n",
    "#### \t•\tImportance:\n",
    "#### \t•\tEnsures data quality and compliance with regulations.\n",
    "#### \t•\tReduces data silos.\n",
    "#### \t•\tEnhances decision-making with reliable data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500f84cd-db85-4f7a-9f89-70fb3eb17e9e",
   "metadata": {},
   "source": [
    "#### 55. What are the main challenges in implementing data governance?\n",
    "#### Answer:\n",
    "#### \t•\tLack of executive support and cultural resistance.\n",
    "#### \t•\tPoor data quality and siloed systems.\n",
    "#### \t•\tDifficulty in aligning policies across departments.\n",
    "#### \t•\tComplex regulatory environments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437507f8-64b4-4bd2-8589-90d01f74cecd",
   "metadata": {},
   "source": [
    "#### 56. What is data lineage, and why is it important?\n",
    "#### Answer:\n",
    "#### \t•\tData lineage traces the flow of data from its origin to its final destination.\n",
    "#### \t•\tImportance:\n",
    "#### \t•\tProvides visibility into data transformations.\n",
    "#### \t•\tHelps in troubleshooting and impact analysis.\n",
    "#### \t•\tEnsures compliance with regulations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f04b4f6-06d4-47a0-b08e-76d01c547282",
   "metadata": {},
   "source": [
    "#### 57. What is the role of metadata management in data governance?\n",
    "#### Answer:\n",
    "#### \t•\tMetadata management involves creating and maintaining a repository of information about data (e.g., its source, structure, and transformations).\n",
    "#### \t•\tIt improves data discoverability, quality, and compliance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999ad686-a8dc-422c-856a-09ce85d9894b",
   "metadata": {},
   "source": [
    "#### 58. How do you measure the success of a data governance program?\n",
    "#### Answer:\n",
    "#### \t•\tMetrics for success:\n",
    "#### \t•\tImprovement in data quality scores.\n",
    "#### \t•\tReduction in data breaches.\n",
    "#### \t•\tIncreased compliance with regulations.\n",
    "#### \t•\tEnhanced user satisfaction with data availability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1d4a8a-07e7-4d1c-8914-5a54013883f5",
   "metadata": {},
   "source": [
    "#### 59. What are the key components of a data governance framework?\n",
    "#### Answer:\n",
    "#### \t•\tData Ownership: Clear ownership of data assets.\n",
    "#### \t•\tPolicies and Standards: Guidelines for data usage and handling.\n",
    "#### \t•\tData Quality Management: Ensures accurate, complete, and consistent data.\n",
    "#### \t•\tData Security and Privacy: Protect data from breaches and ensure compliance.\n",
    "#### \t•\tMetadata Management: Centralizes information about data (e.g., source, structure, purpose)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8779a0c8-96da-45e4-ac24-d5ba2e6410f2",
   "metadata": {},
   "source": [
    "#### 60. How do you balance data accessibility and security in governance?\n",
    "#### Answer:\n",
    "#### \t•\tUse role-based access control (RBAC) to limit data access based on roles.\n",
    "#### \t•\tEncrypt sensitive data and anonymize it when sharing.\n",
    "#### \t•\tMonitor access logs for unauthorized activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a711df9-d5fc-44eb-b8c3-1f2b2a439f72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
